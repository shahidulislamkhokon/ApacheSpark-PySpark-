{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNwImETZKzDCjBqDveUNzKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahidulislamkhokon/ApacheSpark-PySpark-/blob/main/PySpark_Handling_Missing_values.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark Handling Missing Values:**\n",
        "- Dropping Columns\n",
        "- Dropping Rows\n",
        "- Various Prameer in Dropping Functionalities\n",
        "- Handling Missing values by Mean, Median and Mode"
      ],
      "metadata": {
        "id": "kYfzGbwDgLCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "R94tJoFbg50d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NuRm-uYf99P"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('Practise').getOrCreate()"
      ],
      "metadata": {
        "id": "oVWDcFukg5yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark = spark.read.csv('test_with_missing_value.csv', header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "eMI1JnAzg523"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "jPsUjOQVe0jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop the colums**"
      ],
      "metadata": {
        "id": "fGSlcFIheO7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.drop('Name').show()"
      ],
      "metadata": {
        "id": "IaySuZ9Hg54t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can drop a row if there have at list one null value or all row is null\n",
        "- how = any (if you want ot drop a row if there is at least one null value)\n",
        "- how = all (if all row are null than you should use all)"
      ],
      "metadata": {
        "id": "BW-bHpbajZG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### any=how\n",
        "df_pyspark.na.drop(how='all').show()"
      ],
      "metadata": {
        "id": "YeZFOrFgg59a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### any=how\n",
        "df_pyspark.na.drop(how='any').show()"
      ],
      "metadata": {
        "id": "j6S8PgQBg5_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otherwise you can delete if there is at least threshold amount of non-null value (for instance, 2 or 3 non-null value in a row)"
      ],
      "metadata": {
        "id": "CsbfLhGKpBmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### theshold\n",
        "df_pyspark.na.drop(how='any', thresh=3).show()"
      ],
      "metadata": {
        "id": "Ho2L0ynOg6CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, if you want to delete null value w.r.t column than you should use **Subset** parameter in the drop function"
      ],
      "metadata": {
        "id": "ESe-eue-q0E1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### subset\n",
        "df_pyspark.na.drop(how='any', subset=['Name']).show()"
      ],
      "metadata": {
        "id": "L2W8zpTHg6Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fill**ing the Missing Value"
      ],
      "metadata": {
        "id": "ep5-p34UrrMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.na.fill('Missing value').show() ## replace null value with your own value"
      ],
      "metadata": {
        "id": "dZtjFuM1rxay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.na.fill('Missing value', 'profession').show() ## replace specific column"
      ],
      "metadata": {
        "id": "-CodHBgsrxdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Missing values by Mean, Median and Mode**"
      ],
      "metadata": {
        "id": "bCn0Lsoqu4Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "m_jNrSWGrxff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Imputer\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols=['Age', 'Experience'],\n",
        "    outputCols=[\"{}_imputed\".format(c) for c in ['Age', 'Experience']]\n",
        ").setStrategy('mode')"
      ],
      "metadata": {
        "id": "ccPynWwPrxha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer.fit(df_pyspark).transform(df_pyspark).show()"
      ],
      "metadata": {
        "id": "MXT2J8Kfrxk8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}