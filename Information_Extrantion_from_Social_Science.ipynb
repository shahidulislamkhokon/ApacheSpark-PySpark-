{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMcvnl4LqO4zGcfgFdCH0Dr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahidulislamkhokon/ApacheSpark-PySpark-/blob/main/Information_Extrantion_from_Social_Science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information Extraction for Social Science Research"
      ],
      "metadata": {
        "id": "PF3rCEvvV9eP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "installing spaCy and other dependencies for this project"
      ],
      "metadata": {
        "id": "yzDc7NUUWKAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXOsHE4eUg7l"
      },
      "outputs": [],
      "source": [
        "!nvcc --version\n",
        "!pip install --upgrade spacy\n",
        "!pip install --upgrade spacy[cuda111,transformers]\n",
        "!pip install jsonlines\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!wget https://andrewhalterman.com/files/cleaned_masdar.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to use two techniques for information extraction: named entity recognition and rule-based extraction using dependency parses. The plan is to:\n",
        "\n",
        "- get started with some hands-on named entity recognition\n",
        "- step back and discuss information extraction and structured prediction at a higher level\n",
        "- return to NER with some applications on real text\n",
        "- next, use dependency parses and custom rules as a technique for information extraction\n",
        "- conclude with some thoughts about extentions"
      ],
      "metadata": {
        "id": "z-7JpSR9XmRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting started with **NER** and **spaCy**"
      ],
      "metadata": {
        "id": "PfAJnYIBYTPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "import jsonlines\n",
        "import re\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "# assert spacy.__version__ == \"3.1.3\""
      ],
      "metadata": {
        "id": "4R306_IEWkgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For spacy I need a pretrain model for prcessing document.\n",
        "- large model (pre-train model with more example)\n",
        "- small model (without pre-train word embedding)\n",
        "- trf model (transformer-based model)\n",
        "I am using large model (en_core_web_trf) and also small model(en_core_web_sm) for comparision"
      ],
      "metadata": {
        "id": "bkKZY2uacYu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "nlp_sm = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "d5lpjvn8d2vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I'll load in a collection of news stories from a local **pro-government newspaper** in **Syri**a called al-Masdar. The articles here primarily describe the **civil war in Syria in 2016 and 2017**"
      ],
      "metadata": {
        "id": "oxgc9XZReBN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(\"cleaned_masdar.jsonl\", \"r\") as f:\n",
        "    articles = list(f.iter())\n"
      ],
      "metadata": {
        "id": "W1EBaP4EeCQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range (len(articles)):\n",
        "#   print(articles[i])\n",
        "# print(len(articles))"
      ],
      "metadata": {
        "id": "CIXtxElSXKF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = articles[100]\n",
        "article"
      ],
      "metadata": {
        "id": "eJyYcEjLemse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To process a document with **spaCy**, I'll use the **nlp object** we instatiated earlier and pass a piece of text to it. The **nlp object** returns a **Document class object**, which has both document and token-level attributes."
      ],
      "metadata": {
        "id": "uU44BmXXfAAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(article['body'])\n",
        "print(doc)\n",
        "# take a look at how many words in a document\n",
        "len(doc)"
      ],
      "metadata": {
        "id": "us7Ypj6ufKKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look document-level attributes\n",
        "dir(doc)"
      ],
      "metadata": {
        "id": "fsNdI6bNfVJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens in a document can by accessed by their number:\n",
        "print(doc[34])\n",
        "# dir(doc[5])"
      ],
      "metadata": {
        "id": "v9TJvajHfcp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the attributes it assigns is **named entity** information for the document. Using **spaCy's built-in visualizer**, we can see all the detected named entities in the document."
      ],
      "metadata": {
        "id": "lzyja1V7fleu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "jxZgjR5sfmI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "just_text = [i['body'] for i in articles]\n",
        "docs = list(tqdm(nlp.pipe(just_text), total=len(just_text)))\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "rW4nlDRD127d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information Extraction and Structured Prediction"
      ],
      "metadata": {
        "id": "cbpC3B1h59r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in doc[0:]:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "ww-FMR0B8AiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "[(i.text, i.ent_iob_ + \"-\" + i.ent_type_) for i in doc[0:]]\n"
      ],
      "metadata": {
        "id": "QTWR_4Yk5_i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models for NER"
      ],
      "metadata": {
        "id": "jJkXT7iZHlta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's identify which organization are mentioned most in our corpus"
      ],
      "metadata": {
        "id": "7c6zfw2KSc2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_orgs = []\n",
        "for d in docs:\n",
        "    orgs = [ent.text for ent in d.ents if ent.label_ == \"PERSON\"]\n",
        "    all_orgs.extend(orgs)\n",
        "    \n",
        "Counter(all_orgs).most_common(15)"
      ],
      "metadata": {
        "id": "PRMrB7aQHmyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which organizations are mentioned most alongside mentions of \"ceasefires\" or \"negotiations\"?"
      ],
      "metadata": {
        "id": "2bAMSof9TOfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title \n",
        "negotiation_orgs = []\n",
        "for d in docs:\n",
        "    for ent in d.ents:\n",
        "        if ent.label_ != \"ORG\":\n",
        "            continue\n",
        "        if re.search(\"negotiat|ceasefire|talks\", ent.sent.text):\n",
        "            negotiation_orgs.append(ent.text)\n",
        "            \n",
        "negotiation_orgs            "
      ],
      "metadata": {
        "id": "wUp5pHB7Sdrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency parses"
      ],
      "metadata": {
        "id": "WFJy1gGtBRMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(articles[313]['body'])\n",
        "sent = list(doc.sents)[1]\n",
        "displacy.render(sent, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "rDPKbkhjBVqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# doc = \"Al-Aqsa Mosque is located in close proximity to various historical and holy sites in Judaism and Christianity, most notably that of the Temple in Jerusalem.\"\n",
        "# doc = nlp(doc)\n",
        "# print(doc)\n",
        "# # take a look at how many words in a document\n",
        "# len(doc)\n",
        "# displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "SEW2K5hrXWGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example information extraction with dependency parses\n",
        "On its own, a dependency parse doesn't give you the ability to extract information from documents. That said, the information within a dependency parse can help you with a rule-based for extracting information.\n",
        "\n",
        "One thing we might want to be able to extract from text is generally what kinds of behaviors or actions are occurring in a particular location. Let's write a function to identify verbs + direct objects that are grammatically linked to a location."
      ],
      "metadata": {
        "id": "hCVN0GTfDaUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc)\n",
        "tok = doc[21]\n",
        "print(\"tok: \",tok)\n",
        "\n",
        "\n",
        "def loc_to_verb(tok):\n",
        "    verb_phrase = []\n",
        "    # first, iterate through all the ancesters of the token\n",
        "    for i in tok.ancestors:\n",
        "        # when you get to a verb (using a POS tag)...\n",
        "        if i.pos_ == \"VERB\":\n",
        "            # ...add the verb to the verb phrase list\n",
        "            verb_phrase.append(i)\n",
        "            # then, also add the direct object(s) of the verb, as long as the original token\n",
        "            # is in the same subtree as the direct object\n",
        "            verb_phrase.extend([j for j in i.children if j.dep_ == \"dobj\" and tok in i.subtree])\n",
        "            # we only want the first verb, so stop after we find one\n",
        "            print(\"verb_phrase\", verb_phrase)\n",
        "            break\n",
        "    # expand out the verb phrase to get modifiers (\"amod\") of the direct object\n",
        "    for i in verb_phrase:\n",
        "        for j in i.children:\n",
        "            if j.dep_ == \"amod\":\n",
        "                verb_phrase.append(j)\n",
        "\n",
        "    # sort the tokens by their position in the original sentence\n",
        "    new_list = sorted(verb_phrase, key=lambda x: x.i)\n",
        "    # join them together with the correct whitespace and return\n",
        "    return ''.join([i.text_with_ws for i in new_list]).strip()\n",
        "\n",
        "loc_to_verb(tok)"
      ],
      "metadata": {
        "id": "gT-wZ5xEDXYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then use our function to identify all the actions related to a single city, Aleppo."
      ],
      "metadata": {
        "id": "Mp7FMrArc2H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aleppo_actions = []\n",
        "\n",
        "for d in docs:\n",
        "    for i in d:\n",
        "        if i.text == \"Aleppo\":\n",
        "            aleppo_actions.append(loc_to_verb(i))\n",
        "\n",
        "sorted(list(set(aleppo_actions)))"
      ],
      "metadata": {
        "id": "Jt3NOLekg1Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_phrase(subtree):\n",
        "    \"\"\"Sort and join tokens into a string\"\"\"\n",
        "    new_list = sorted(list(subtree), key=lambda x: x.i)\n",
        "    return ''.join([i.text_with_ws for i in new_list])\n",
        "\n",
        "\n",
        "for i in doc:\n",
        "    # Find instances of the word \"backed\" that play the role of an adjectival modifier\n",
        "    if i.text == \"backed\" and i.dep_ == \"amod\":\n",
        "        # The children of \"backed\" will report who the backer is\n",
        "        print(\"Backer: \", clean_phrase(i.children))\n",
        "        # Next, we go up one level to the immediate parent of the word \"backed\"\n",
        "        parent = list(i.ancestors)[0]\n",
        "        branches = [parent]\n",
        "        # for each of the children of that word, except for the original \"backed\" token,\n",
        "        # add it to the branch\n",
        "        for j in parent.children:\n",
        "            if j != i:\n",
        "                branches.append(j)\n",
        "\n",
        "        print(branches)\n",
        "    \n"
      ],
      "metadata": {
        "id": "cb2Mi1m0lvt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extentions and Experiments"
      ],
      "metadata": {
        "id": "g4Gc6CPfmSqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf\n",
        "\n",
        "nlp_trf = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "doc = nlp_trf(articles[313]['body'])\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "TOPwuWyBmVgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "hugg = pipeline('question-answering', model=model_name, tokenizer=model_name)"
      ],
      "metadata": {
        "id": "GeDgGD2gmeXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QA_input = {\n",
        "    'question': \"Who controls Deir Hafer and Al-Bab?\",\n",
        "    'context': sent.text\n",
        "}\n",
        "res = hugg(QA_input)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "uFBbOwHpnHEn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}